{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6557549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 SGM files\n",
      "\n",
      "Reading reut2-000.sgm...\n",
      "  Found 1000 documents\n",
      "Reading reut2-001.sgm...\n",
      "  Found 1000 documents\n",
      "Reading reut2-002.sgm...\n",
      "  Found 1000 documents\n",
      "\n",
      "Total documents: 3000\n",
      "================================================================================\n",
      "\n",
      "Sample Documents:\n",
      "\n",
      "[Document 1] ID: 1\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:01:01.79\n",
      "Title: BAHIA COCOA REVIEW\n",
      "Dateline: SALVADOR, Feb 26 -\n",
      "Body: Showers continued throughout the week in\n",
      "the Bahia cocoa zone, alleviating the drought since early\n",
      "January and improving prospects for the coming temp...\n",
      "Places: el-salvador, usa, uruguay\n",
      "Topics: cocoa\n",
      "\n",
      "[Document 2] ID: 2\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:02:20.00\n",
      "Title: STANDARD OIL <SRD> TO FORM FINANCIAL UNIT\n",
      "Dateline: CLEVELAND, Feb 26 -\n",
      "Body: Standard Oil Co and BP North America\n",
      "Inc said they plan to form a venture to manage the money market\n",
      "borrowing and investment activities of both compa...\n",
      "Places: usa\n",
      "\n",
      "[Document 3] ID: 3\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:03:27.51\n",
      "Title: TEXAS COMMERCE BANCSHARES <TCB> FILES PLAN\n",
      "Dateline: HOUSTON, Feb 26 -\n",
      "Body: Texas Commerce Bancshares Inc's Texas\n",
      "Commerce Bank-Houston said it filed an application with the\n",
      "Comptroller of the Currency in an effort to create t...\n",
      "Places: usa\n",
      "\n",
      "[Document 4] ID: 4\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:07:13.72\n",
      "Title: TALKING POINT/BANKAMERICA <BAC> EQUITY OFFER\n",
      "Dateline: LOS ANGELES, Feb 26 -\n",
      "Body: BankAmerica Corp is not under\n",
      "pressure to act quickly on its proposed equity offering and\n",
      "would do well to delay it because of the stock's recent poor...\n",
      "Places: usa, brazil\n",
      "\n",
      "[Document 5] ID: 5\n",
      "--------------------------------------------------------------------------------\n",
      "Date: 26-FEB-1987 15:10:44.60\n",
      "Title: NATIONAL AVERAGE PRICES FOR FARMER-OWNED RESERVE\n",
      "Dateline: WASHINGTON, Feb 26 -\n",
      "Body: The U.S. Agriculture Department\n",
      "reported the farmer-owned reserve national five-day average\n",
      "price through February 25 as follows (Dlrs/Bu-Sorghum Cwt)...\n",
      "Places: usa\n",
      "Topics: grain, wheat, corn, barley, oat, sorghum\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "--------------------------------------------------------------------------------\n",
      "Documents with body text: 2761/3000 (92.0%)\n",
      "Documents with places: 2714/3000 (90.5%)\n",
      "Documents with topics: 1600/3000 (53.3%)\n",
      "Documents with people: 129/3000 (4.3%)\n",
      "Documents with organizations: 143/3000 (4.8%)\n",
      "Documents with companies: 0/3000 (0.0%)\n",
      "\n",
      "Unique places (106): ['afghanistan', 'algeria', 'argentina', 'australia', 'austria', 'bahrain', 'bangladesh', 'belgium', 'bhutan', 'bolivia', 'brazil', 'canada', 'cayman-islands', 'chile', 'china', 'colombia', 'congo', 'costa-rica', 'cuba', 'czechoslovakia']\n",
      "  ... and 86 more\n",
      "\n",
      "Unique topics (88): ['acq', 'alum', 'barley', 'bop', 'can', 'carcass', 'citruspulp', 'cocoa', 'coffee', 'copper', 'copra-cake', 'corn', 'cornglutenfeed', 'cotton', 'cpi', 'crude', 'dlr', 'earn', 'fishmeal', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'linseed', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'nat-gas', 'nickel', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'plywood', 'potato', 'propane', 'rape-meal', 'rape-oil', 'rapeseed', 'red-bean', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'saudriyal', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-oil', 'sunseed', 'tapioca', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wool', 'wpi', 'yen', 'zinc']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Key Fields for the Project:\n",
      "--------------------------------------------------------------------------------\n",
      "Title - for autocomplete and search\n",
      "Body - main content for text analysis\n",
      "Date - temporal information (needs parsing)\n",
      "Places - geographic tags (need geocoding to lat/lon)\n",
      "Dateline - contains city names (extra location info)\n",
      "Topics - subject categories\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "def parse_reuters_document(text):\n",
    "    # extract reuters articles from text\n",
    "    pattern = r'<REUTERS.*?</REUTERS>'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    return matches\n",
    "\n",
    "def clean_xml(xml_text):\n",
    "    # remove invalid xml characters\n",
    "    xml_text = re.sub(r'&#\\d+;', '', xml_text)\n",
    "    return xml_text\n",
    "\n",
    "def extract_fields(reuters_xml):\n",
    "    # parse xml and get fields\n",
    "    try:\n",
    "        cleaned = clean_xml(reuters_xml)\n",
    "        root = ET.fromstring(cleaned)\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Parse error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    doc = {\n",
    "        'newid': root.get('NEWID'),\n",
    "        'date': root.findtext('DATE', '').strip(),\n",
    "        'title': '',\n",
    "        'dateline': '',\n",
    "        'body': '',\n",
    "        'topics': [],\n",
    "        'places': [],\n",
    "        'people': [],\n",
    "        'orgs': [],\n",
    "        'companies': []\n",
    "    }\n",
    "    \n",
    "    # get text content\n",
    "    text_elem = root.find('TEXT')\n",
    "    if text_elem is not None:\n",
    "        doc['title'] = text_elem.findtext('TITLE', '').strip()\n",
    "        doc['dateline'] = text_elem.findtext('DATELINE', '').strip()\n",
    "        doc['body'] = text_elem.findtext('BODY', '').strip()\n",
    "    \n",
    "    # get categories\n",
    "    topics_elem = root.find('TOPICS')\n",
    "    if topics_elem is not None:\n",
    "        doc['topics'] = [d.text for d in topics_elem.findall('D') if d.text]\n",
    "    \n",
    "    places_elem = root.find('PLACES')\n",
    "    if places_elem is not None:\n",
    "        doc['places'] = [d.text for d in places_elem.findall('D') if d.text]\n",
    "    \n",
    "    people_elem = root.find('PEOPLE')\n",
    "    if people_elem is not None:\n",
    "        doc['people'] = [d.text for d in people_elem.findall('D') if d.text]\n",
    "    \n",
    "    orgs_elem = root.find('ORGS')\n",
    "    if orgs_elem is not None:\n",
    "        doc['orgs'] = [d.text for d in orgs_elem.findall('D') if d.text]\n",
    "    \n",
    "    companies_elem = root.find('COMPANIES')\n",
    "    if companies_elem is not None:\n",
    "        doc['companies'] = [d.text for d in companies_elem.findall('D') if d.text]\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# main execution\n",
    "data_path = r\"data\"\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "# find all sgm files\n",
    "sgm_files = [f for f in os.listdir(data_path) if f.endswith('.sgm')]\n",
    "print(f\"Found {len(sgm_files)} SGM files\\n\")\n",
    "\n",
    "# read each file\n",
    "for filename in sgm_files[:3]:\n",
    "    filepath = os.path.join(data_path, filename)\n",
    "    print(f\"Reading {filename}...\")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    docs = parse_reuters_document(content)\n",
    "    all_documents.extend(docs)\n",
    "    print(f\"  Found {len(docs)} documents\")\n",
    "\n",
    "print(f\"\\nTotal documents: {len(all_documents)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# show sample documents\n",
    "print(\"\\nSample Documents:\")\n",
    "for i, doc_xml in enumerate(all_documents[:5]):\n",
    "    doc = extract_fields(doc_xml)\n",
    "    \n",
    "    if doc is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n[Document {i+1}] ID: {doc['newid']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Date: {doc['date']}\")\n",
    "    print(f\"Title: {doc['title']}\")\n",
    "    print(f\"Dateline: {doc['dateline']}\")\n",
    "    \n",
    "    body_preview = doc['body'][:150] + \"...\" if len(doc['body']) > 150 else doc['body']\n",
    "    print(f\"Body: {body_preview}\")\n",
    "    \n",
    "    if doc['places']:\n",
    "        print(f\"Places: {', '.join(doc['places'])}\")\n",
    "    if doc['topics']:\n",
    "        print(f\"Topics: {', '.join(doc['topics'])}\")\n",
    "    if doc['people']:\n",
    "        print(f\"People: {', '.join(doc['people'])}\")\n",
    "    if doc['orgs']:\n",
    "        print(f\"Organizations: {', '.join(doc['orgs'])}\")\n",
    "    if doc['companies']:\n",
    "        print(f\"Companies: {', '.join(doc['companies'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# calculate statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "all_docs = [extract_fields(doc_xml) for doc_xml in all_documents]\n",
    "all_docs = [d for d in all_docs if d is not None]\n",
    "\n",
    "docs_with_topics = sum(1 for d in all_docs if d['topics'])\n",
    "docs_with_places = sum(1 for d in all_docs if d['places'])\n",
    "docs_with_people = sum(1 for d in all_docs if d['people'])\n",
    "docs_with_orgs = sum(1 for d in all_docs if d['orgs'])\n",
    "docs_with_companies = sum(1 for d in all_docs if d['companies'])\n",
    "docs_with_body = sum(1 for d in all_docs if d['body'])\n",
    "\n",
    "total = len(all_docs)\n",
    "print(f\"Documents with body text: {docs_with_body}/{total} ({100*docs_with_body/total:.1f}%)\")\n",
    "print(f\"Documents with places: {docs_with_places}/{total} ({100*docs_with_places/total:.1f}%)\")\n",
    "print(f\"Documents with topics: {docs_with_topics}/{total} ({100*docs_with_topics/total:.1f}%)\")\n",
    "print(f\"Documents with people: {docs_with_people}/{total} ({100*docs_with_people/total:.1f}%)\")\n",
    "print(f\"Documents with organizations: {docs_with_orgs}/{total} ({100*docs_with_orgs/total:.1f}%)\")\n",
    "print(f\"Documents with companies: {docs_with_companies}/{total} ({100*docs_with_companies/total:.1f}%)\")\n",
    "\n",
    "# show unique values\n",
    "all_places = set()\n",
    "all_topics = set()\n",
    "for d in all_docs:\n",
    "    all_places.update(d['places'])\n",
    "    all_topics.update(d['topics'])\n",
    "\n",
    "print(f\"\\nUnique places ({len(all_places)}): {sorted(list(all_places))[:20]}\")\n",
    "if len(all_places) > 20:\n",
    "    print(f\"  ... and {len(all_places) - 20} more\")\n",
    "\n",
    "print(f\"\\nUnique topics ({len(all_topics)}): {sorted(list(all_topics))}\")\n",
    "\n",
    "# important fields summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nKey Fields for the Project:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Title - for autocomplete and search\")\n",
    "print(\"Body - main content for text analysis\")\n",
    "print(\"Date - temporal information (needs parsing)\")\n",
    "print(\"Places - geographic tags (need geocoding to lat/lon)\")\n",
    "print(\"Dateline - contains city names (extra location info)\")\n",
    "print(\"Topics - subject categories\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1 (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
